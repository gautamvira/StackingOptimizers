# StackingOptimizers
Testing a combination of optimizers for neural networks for better convergence and generalization.  
Final results suggest that combining Adam and SGD with momentum during the training phase provide better scores.
